from pocketflow import Node
from utils.authenticity_scorer import score_authenticity

class AuthenticityScorerNode(Node):
    """
    Evaluates human-like qualities beyond pattern avoidance.
    Node that provides final authenticity assessment and ROI analysis.
    """
    
    def prep(self, shared):
        """
        Read content_pieces, brand_voice, and quality_control from shared store.
        """
        return {
            "content_pieces": shared["content_pieces"],
            "brand_voice": shared["brand_voice"],
            "quality_control": shared["quality_control"],
            "version_history": shared["version_history"]
        }
    
    def exec(self, prep_res):
        """
        Advanced authenticity evaluation with quantifiable metrics and ROI analysis.
        """
        content_pieces = prep_res["content_pieces"]
        brand_voice = prep_res["brand_voice"]
        quality_control = prep_res["quality_control"]
        version_history = prep_res["version_history"]
        
        # Score authenticity for each platform
        authenticity_results = {}
        overall_metrics = {
            "pattern_elimination": [],
            "natural_flow": [],
            "engagement_quality": [],
            "brand_alignment": [],
            "human_quality": []
        }
        
        for platform, content in content_pieces.items():
            print(f"Scoring authenticity for {platform}...")
            
            # Extract content text
            content_text = self._get_content_text(content, platform)
            
            # Get pattern history for this platform
            pattern_history = quality_control.get("deadly_sins_violations", {}).get(platform, {})
            
            # Score authenticity
            score_result = score_authenticity(
                text=content_text,
                brand_voice=brand_voice["parsed_attributes"],
                pattern_history=pattern_history
            )
            
            authenticity_results[platform] = score_result
            
            # Collect metrics for overall calculation
            for metric_name in overall_metrics.keys():
                if metric_name in score_result["metrics"]:
                    overall_metrics[metric_name].append(score_result["metrics"][metric_name]["score"])
        
        # Calculate overall authenticity score
        overall_scores = {}
        for metric_name, scores in overall_metrics.items():
            if scores:
                overall_scores[metric_name] = sum(scores) / len(scores)
            else:
                overall_scores[metric_name] = 0.0
        
        # Weighted overall score
        weights = {
            "pattern_elimination": 0.3,
            "natural_flow": 0.2,
            "engagement_quality": 0.2,
            "brand_alignment": 0.15,
            "human_quality": 0.15
        }
        
        overall_authenticity_score = sum(
            overall_scores[metric] * weights[metric] 
            for metric in weights.keys()
        )
        
        # Generate comprehensive ROI analysis
        roi_analysis = self._generate_comprehensive_roi_analysis(
            authenticity_results, 
            version_history, 
            quality_control
        )
        
        return {
            "authenticity_results": authenticity_results,
            "overall_scores": overall_scores,
            "overall_authenticity_score": overall_authenticity_score,
            "weights": weights,
            "roi_analysis": roi_analysis
        }
    
    def _get_content_text(self, content, platform):
        """
        Extract text content from platform-specific structure.
        """
        if isinstance(content, str):
            return content
        elif isinstance(content, dict):
            if platform == "email":
                return f"{content.get('subject', '')}\n\n{content.get('body', '')}"
            elif platform in ["linkedin", "instagram"]:
                return f"{content.get('text', '')} {' '.join(content.get('hashtags', []))}"
            elif platform == "twitter":
                if "thread" in content:
                    return " ".join(content["thread"])
                else:
                    return content.get("tweet", "")
            elif platform == "reddit":
                return f"{content.get('title', '')}\n\n{content.get('body', '')}"
            elif platform == "blog":
                return f"{content.get('title', '')}\n\n{content.get('body', '')}"
            else:
                return str(content)
        else:
            return str(content)
    
    def _generate_comprehensive_roi_analysis(self, authenticity_results, version_history, quality_control):
        """
        Generate comprehensive ROI analysis based on authenticity scores and revision history.
        """
        # Calculate average authenticity score
        scores = [result["authenticity_score"] for result in authenticity_results.values()]
        avg_authenticity = sum(scores) / len(scores) if scores else 0
        
        # Calculate pattern reduction
        total_violations_initial = 0
        total_violations_final = 0
        
        for platform_results in quality_control.get("deadly_sins_violations", {}).values():
            total_violations_final += platform_results.get("total_violations", 0)
        
        # Estimate initial violations (this would come from version history in real implementation)
        total_violations_initial = total_violations_final * 3 if total_violations_final > 0 else 10
        
        pattern_reduction_rate = 0
        if total_violations_initial > 0:
            pattern_reduction_rate = ((total_violations_initial - total_violations_final) / total_violations_initial) * 100
        
        # Time savings estimation
        time_saved_per_piece = (avg_authenticity / 100) * 20  # minutes
        revision_count = quality_control.get("revision_count", 0)
        time_saved_from_revisions = revision_count * 5  # minutes per revision avoided
        
        # Quality improvement
        quality_improvement = (avg_authenticity - 50) / 50 if avg_authenticity > 50 else 0
        
        return {
            "time_saved_per_piece_minutes": round(time_saved_per_piece, 1),
            "time_saved_from_revisions": round(time_saved_from_revisions, 1),
            "total_time_saved": round(time_saved_per_piece + time_saved_from_revisions, 1),
            "quality_improvement_percent": round(quality_improvement * 100, 1),
            "pattern_reduction_rate": round(max(0, pattern_reduction_rate), 1),
            "estimated_roi": f"{round(avg_authenticity * 2, 1)}%",
            "content_pieces_processed": len(authenticity_results),
            "average_authenticity_score": round(avg_authenticity, 1),
            "revision_cycles_avoided": max(0, 3 - revision_count)  # Assuming 3 is typical manual revision count
        }
    
    def post(self, shared, prep_res, exec_res):
        """
        Store authenticity scores and ROI analysis in shared store.
        """
        # Update shared store with authenticity results
        shared["authenticity_results"] = exec_res["authenticity_results"]
        shared["roi_analysis"] = exec_res["roi_analysis"]
        
        # Mark stage as completed
        shared["workflow_state"]["completed_stages"].append("authenticity_scoring")
        shared["workflow_state"]["current_stage"] = "feedback_routing"
        
        print(f"\nAuthenticity scoring completed!")
        print(f"Overall authenticity score: {exec_res['overall_authenticity_score']:.2f}/100")
        print(f"Content pieces analyzed: {exec_res['roi_analysis']['content_pieces_processed']}")
        
        # Show detailed scores
        print(f"\nPlatform Scores:")
        for platform, score_data in exec_res["authenticity_results"].items():
            print(f"  {platform.title()}: {score_data['authenticity_score']}/100")
        
        # Show ROI analysis
        roi = exec_res["roi_analysis"]
        print(f"\nROI Analysis:")
        print(f"  Time saved per piece: {roi['time_saved_per_piece_minutes']} minutes")
        print(f"  Revisions avoided: {roi['revision_cycles_avoided']}")
        print(f"  Total time saved: {roi['total_time_saved']} minutes")
        print(f"  Pattern reduction: {roi['pattern_reduction_rate']}%")
        print(f"  Estimated ROI: {roi['estimated_roi']}")
        
        # Show feedback if any issues detected
        issues_found = False
        for platform, score_data in exec_res["authenticity_results"].items():
            if score_data["authenticity_score"] < 85:
                issues_found = True
                feedback = score_data.get("feedback", [])
                if feedback:
                    print(f"\n{platform.title()} recommendations:")
                    for item in feedback[:3]:  # Show top 3
                        print(f"  - {item}")
        
        if not issues_found:
            print(f"\nâœ… All content meets high authenticity standards!")
        
        return "continue"  # Go to next node

if __name__ == "__main__":
    # Test the node
    node = AuthenticityScorerNode()
    
    # Create test shared store
    shared = {
        "content_pieces": {
            "linkedin": {
                "text": "AI is transforming marketing in exciting ways. The future holds incredible possibilities for brands that embrace innovation. #AI #Marketing",
                "hashtags": ["#AI", "#Marketing"]
            },
            "twitter": {
                "tweet": "Exciting developments in AI marketing! The future is bright. #AI #Tech"
            }
        },
        "brand_voice": {
            "parsed_attributes": {
                "personality_traits": ["innovative", "professional"],
                "tone": "thought_leadership",
                "voice": "confident",
                "values": ["innovation", "excellence"],
                "themes": ["AI", "technology"]
            }
        },
        "quality_control": {
            "deadly_sins_violations": {
                "linkedin": {"total_violations": 0},
                "twitter": {"total_violations": 0}
            },
            "revision_count": 1,
            "max_revisions_reached": False,
            "compliance_status": "pass"
        },
        "version_history": [
            {
                "version_id": "v1",
                "timestamp": "2024-01-01T10:00:00Z",
                "violations_fixed": ["em_dash"],
                "changes_made": ["removed_em_dash"]
            }
        ],
        "workflow_state": {
            "current_stage": "authenticity_scoring",
            "completed_stages": ["engagement", "brand_bible_processing", "format_guidelines", "creative_inspiration", "pattern_aware_prompt_engineering", "content_generation", "style_optimization", "deadly_sins_scanning", "style_compliance"],
            "error_state": None
        }
    }
    
    print("Testing AuthenticityScorerNode...")
    action = node.run(shared)
    print(f"Action: {action}")
    print(f"Authenticity results: {list(shared['authenticity_results'].keys())}")
    
    if 'authenticity_results' in shared:
        for platform, result in shared['authenticity_results'].items():
            print(f"{platform.title()} authenticity: {result['authenticity_score']}/100")
